{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "##  Project: Image Classification \n",
    "\n",
    "In this project, I have classified images from the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html). The dataset consists of airplanes, dogs, cats, and other objects. \n",
    "\n",
    "In this project I have used Keras a framework which supports convolutional neural network for classifying images form CIFAR-10 dataset. I have developed my onw model based on Keras and used it for training and testing.\n",
    "\n",
    "I have used following steps in the project\n",
    "\n",
    "* Download Keras\n",
    "* Import all the modules\n",
    "* Download the images from CIFAR-10 dataset\n",
    "* Augment the images dataset by performing operations like rotation\n",
    "* Define Keras model\n",
    "* Train and test the dataset\n",
    "\n",
    "\n",
    "I have run this on my laptop (Lenovo Legin Y720) which has 6 GB GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7tCV4NJbPXI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n",
      "tensorflow-gpu 1.9.0 has requirement tensorboard<1.10.0,>=1.9.0, but you'll have tensorboard 1.10.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "E:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DN4vvW7d-Jx"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow configuration changes for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D7He0K8eKqQ"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyper parameters to be used\n",
    "\n",
    "Following hyper parameters are used:\n",
    "\n",
    "* batch_size - Batch size\n",
    "* num_classes - Number of classes (10)\n",
    "* epochs - Number of epochs (250)\n",
    "* l - Number of layers\n",
    "* num_filter - Number of filters \n",
    "* compression\n",
    "* dropout_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x2JuqDceTUG"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 250\n",
    "l = 22\n",
    "num_filter = 16\n",
    "compression = 0.5\n",
    "dropout_rate = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset and perform one hot encoding\n",
    "\n",
    "In this step, I have downloaded the cifar10 training and test data\n",
    "\n",
    "Next, performed one-hot encoding on target labels train_y and test_y to get the resultant train_y, test_y which will be used for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "P9oAHIKLenCA",
    "outputId": "4d4e3790-8d15-4add-ee79-d1393ad8a2cf"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoding \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image augmentation of images\n",
    "\n",
    "1. Peform image augmentation by rotating 45 degrees for a randomly selected 1000 images and append the augmented images\n",
    "2. Perform image augmentation by rotating 45 degrees for a randomly selected 1000 images and append the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Rotate images by 45 degrees\n",
    "datagen1 = ImageDataGenerator(rotation_range=45)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen1.fit(x_train)\n",
    "\n",
    "# Get 1000 augmented images and append to the training data\n",
    "for x_rotated_images_batch, y_rotated_images_batch in datagen1.flow(x_train, y_train, batch_size=1000):\n",
    "    x_train = np.append(x_train, x_rotated_images_batch, axis=0)\n",
    "    y_train = np.append(y_train, y_rotated_images_batch, axis=0)\n",
    "    break\n",
    "\n",
    "# Horizontally flip image\n",
    "datagen2 = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen2.fit(x_train)  \n",
    "\n",
    "# Get 1000 augmented horizontally flipped images and append to the training data\n",
    "for x_flipped_images_batch, y_flipped_images_batch in datagen2.flow(x_train, y_train, batch_size=1000):\n",
    "    x_train = np.append(x_train, x_flipped_images_batch, axis=0)\n",
    "    y_train = np.append(y_train, y_flipped_images_batch, axis=0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create building blocks for model\n",
    "\n",
    "1. Create a method for add_denseblock of layers where each layer has following operations:\n",
    "    perform Batch Normalization, \n",
    "    Perform relu Activation\n",
    "    Apply 3x3 convolution of a number of filters num_filter*compression\n",
    "    Add dropout\n",
    "    \n",
    "    All the layers are concated to form a dense block\n",
    "    \n",
    "2. Create a method add_transition which does the folloiwng:\n",
    "    \n",
    "    perform Batch Normalization, \n",
    "    Perform relu Activation\n",
    "    Apply 1x1 convolution of a number of filters num_filter*compression\n",
    "    Add dropout\n",
    "    Perform 2x2 Average Pooling\n",
    "    \n",
    "3.  Create a method output_layer which does the folloiwng:\n",
    "    \n",
    "    perform Batch Normalization, \n",
    "    Perform relu Activation\n",
    "    Perform 2x2 Average Pooling\n",
    "    Flatten\n",
    "    Dense layer of 10 classes with softmax activation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epgz2n0ve48o"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV4_k7hzfF-C"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return AvgPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvy4cIPPfKo1"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Model\n",
    "\n",
    "\n",
    "Apply  building blocks of dense_block, transtion blocks and 3x3 convolution filters to create the final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK2f9RdpfWRy"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "layer1_transition = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "layer2_block = add_denseblock(layer1_transition, num_filter, dropout_rate)\n",
    "layer2_transition = add_transition(layer2_block, num_filter, dropout_rate)\n",
    "skip_connection1 = layer2_transition\n",
    "\n",
    "layer3_block = add_denseblock(layer2_transition, num_filter, dropout_rate)\n",
    "layer3_transition = add_transition(layer3_block, num_filter, dropout_rate)\n",
    "\n",
    "layer4_block = add_denseblock(layer3_transition, num_filter, dropout_rate)\n",
    "layer4_transition = add_transition(layer4_block, num_filter, dropout_rate)\n",
    "\n",
    "layer5_skip_connection_block = add_denseblock(skip_connection1, num_filter, dropout_rate)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_block = Concatenate(axis=-1)([layer5_skip_connection_block, layer4_transition])\n",
    "layer5_transition = add_transition(layer5_block, num_filter, dropout_rate)\n",
    "\n",
    "layer6_block = add_denseblock(layer5_transition,  num_filter, dropout_rate)\n",
    "output = output_layer(layer6_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10564
    },
    "colab_type": "code",
    "id": "Ur0VMkKKfiMs",
    "outputId": "e2f9e2bb-4455-4348-f4e5-82a496b672ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 8)    1152        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 8)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 24)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 24)   96          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 24)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 8)    1728        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 8)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 32)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 8)    2304        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 8)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 40)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 40)   160         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 40)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 8)    2880        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 8)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 48)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 8)    3456        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 8)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 56)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 56)   224         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 56)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 8)    4032        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 8)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 64)   0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 8)    4608        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 8)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 72)   0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 72)   288         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 72)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 8)    5184        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 8)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 80)   0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 80)   320         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 80)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 8)    5760        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 8)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 88)   0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 88)   352         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 88)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 8)    6336        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 8)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 96)   0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 96)   384         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 8)    6912        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 8)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 104)  0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 104)  416         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 104)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 8)    7488        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 8)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 112)  0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 112)  448         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 112)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 8)    8064        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 8)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 120)  0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 120)  480         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 120)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 8)    8640        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 8)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 128)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 8)    9216        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 8)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 136)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 136)  544         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 136)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 8)    9792        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32, 32, 8)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 144)  0           concatenate_15[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 144)  576         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 144)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 8)    10368       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32, 32, 8)    0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 152)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 152)  608         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 152)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 8)    10944       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32, 32, 8)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 32, 32, 160)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 160)  640         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 160)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 8)    11520       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 32, 8)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 32, 32, 168)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 168)  672         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 168)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 8)    12096       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 32, 8)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 32, 32, 176)  0           concatenate_19[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 176)  704         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 176)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 8)    12672       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32, 32, 8)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 32, 32, 184)  0           concatenate_20[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 184)  736         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 184)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 8)    13248       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32, 32, 8)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 32, 32, 192)  0           concatenate_21[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 192)  768         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 192)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 8)    1536        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32, 32, 8)    0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 8)    0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 8)    32          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 8)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 8)    576         activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 16, 8)    0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 16)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 16)   64          concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 16)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 8)    1152        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 16, 8)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 24)   0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 24)   96          concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 24)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 8)    1728        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 16, 8)    0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 16, 16, 32)   0           concatenate_24[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 8)    2304        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 16, 16, 8)    0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 40)   0           concatenate_25[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 40)   160         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 40)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 8)    2880        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 16, 16, 8)    0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 48)   0           concatenate_26[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 48)   192         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 48)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 8)    3456        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 16, 16, 8)    0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 56)   0           concatenate_27[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 56)   224         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 56)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 8)    4032        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 16, 16, 8)    0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 64)   0           concatenate_28[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 64)   256         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 8)    4608        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 16, 16, 8)    0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 72)   0           concatenate_29[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 72)   288         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 72)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 8)    5184        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 16, 16, 8)    0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 80)   0           concatenate_30[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 80)   320         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 80)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 8)    5760        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 16, 16, 8)    0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 88)   0           concatenate_31[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 88)   352         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 88)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 8)    6336        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 16, 16, 8)    0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 16, 16, 96)   0           concatenate_32[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 96)   384         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 8)    6912        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 16, 16, 8)    0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 16, 16, 104)  0           concatenate_33[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 104)  416         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 104)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 8)    7488        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 16, 16, 8)    0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 16, 16, 112)  0           concatenate_34[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 112)  448         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 112)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 8)    8064        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 16, 16, 8)    0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 16, 16, 120)  0           concatenate_35[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 120)  480         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 120)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 8)    8640        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 16, 16, 8)    0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 16, 16, 128)  0           concatenate_36[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 128)  512         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 8)    9216        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16, 16, 8)    0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 16, 16, 136)  0           concatenate_37[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 136)  544         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 136)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 8)    9792        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 16, 16, 8)    0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 16, 16, 144)  0           concatenate_38[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 144)  576         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 144)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 8)    10368       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 16, 16, 8)    0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 16, 16, 152)  0           concatenate_39[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 152)  608         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 152)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 8)    10944       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 16, 16, 8)    0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 16, 16, 160)  0           concatenate_40[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 160)  640         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 8)    11520       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 16, 16, 8)    0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 16, 16, 168)  0           concatenate_41[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 168)  672         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 168)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 8)    12096       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 16, 16, 8)    0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 16, 16, 176)  0           concatenate_42[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 176)  704         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 176)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 16, 16, 8)    12672       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 16, 16, 8)    0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 16, 16, 184)  0           concatenate_43[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 184)  736         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 184)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 16, 16, 8)    1472        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 16, 16, 8)    0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 8)      0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 8)      32          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 8)      0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 16, 16, 8)    32          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 8)      576         activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 16, 16, 8)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 8, 8, 8)      0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 8)    576         activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 16)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 16, 16, 8)    0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 16)     64          concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 16, 16, 16)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 16)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 16, 16)   64          concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 8)      1152        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 16, 16)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 8, 8, 8)      0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 8)    1152        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 24)     0           concatenate_45[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 16, 16, 8)    0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 24)     96          concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 16, 16, 24)   0           concatenate_67[0][0]             \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 24)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 16, 24)   96          concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 8)      1728        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 16, 16, 24)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 8, 8, 8)      0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 16, 16, 8)    1728        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 32)     0           concatenate_46[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 16, 16, 8)    0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 32)     128         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 16, 16, 32)   0           concatenate_68[0][0]             \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 8, 8, 32)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 16, 32)   128         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 8)      2304        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 16, 16, 32)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 8, 8, 8)      0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 8)    2304        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 8, 8, 40)     0           concatenate_47[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 16, 16, 8)    0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 40)     160         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 16, 16, 40)   0           concatenate_69[0][0]             \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 40)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 40)   160         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 8)      2880        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 16, 40)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 8, 8, 8)      0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 8)    2880        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 8, 8, 48)     0           concatenate_48[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 16, 16, 8)    0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 48)     192         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 16, 16, 48)   0           concatenate_70[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 48)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 16, 48)   192         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 8, 8)      3456        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 16, 48)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 8, 8, 8)      0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 8)    3456        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 8, 8, 56)     0           concatenate_49[0][0]             \n",
      "                                                                 dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 16, 16, 8)    0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 56)     224         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 16, 16, 56)   0           concatenate_71[0][0]             \n",
      "                                                                 dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 8, 8, 56)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 16, 56)   224         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 8)      4032        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 16, 56)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 8, 8, 8)      0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 8)    4032        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 8, 8, 64)     0           concatenate_50[0][0]             \n",
      "                                                                 dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 16, 16, 8)    0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 64)     256         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 16, 16, 64)   0           concatenate_72[0][0]             \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 8, 8, 64)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 16, 16, 64)   256         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 8)      4608        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 64)   0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 8, 8, 8)      0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 8)    4608        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 8, 8, 72)     0           concatenate_51[0][0]             \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 16, 16, 8)    0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 8, 8, 72)     288         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 16, 16, 72)   0           concatenate_73[0][0]             \n",
      "                                                                 dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 8, 8, 72)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 16, 72)   288         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 8)      5184        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 16, 72)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 8, 8, 8)      0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 16, 16, 8)    5184        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 8, 8, 80)     0           concatenate_52[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 16, 16, 8)    0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 8, 8, 80)     320         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 16, 16, 80)   0           concatenate_74[0][0]             \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 8, 8, 80)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 16, 16, 80)   320         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 8, 8, 8)      5760        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 16, 80)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 8, 8, 8)      0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 8)    5760        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 8, 8, 88)     0           concatenate_53[0][0]             \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 16, 16, 8)    0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 8, 8, 88)     352         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 16, 16, 88)   0           concatenate_75[0][0]             \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 8, 8, 88)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 88)   352         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 8, 8, 8)      6336        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 88)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 8, 8, 8)      0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 8)    6336        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 8, 8, 96)     0           concatenate_54[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 16, 16, 8)    0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 8, 8, 96)     384         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 16, 16, 96)   0           concatenate_76[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 8, 8, 96)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 96)   384         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 8, 8, 8)      6912        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 96)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 8, 8, 8)      0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 8)    6912        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 8, 8, 104)    0           concatenate_55[0][0]             \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 16, 16, 8)    0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 8, 104)    416         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 16, 16, 104)  0           concatenate_77[0][0]             \n",
      "                                                                 dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 8, 8, 104)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 104)  416         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 8, 8, 8)      7488        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 16, 104)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 8, 8, 8)      0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 8)    7488        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 8, 8, 112)    0           concatenate_56[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 16, 16, 8)    0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 8, 112)    448         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 16, 16, 112)  0           concatenate_78[0][0]             \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 8, 8, 112)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 112)  448         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 8, 8, 8)      8064        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 16, 16, 112)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 8, 8, 8)      0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 8)    8064        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 8, 8, 120)    0           concatenate_57[0][0]             \n",
      "                                                                 dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 16, 16, 8)    0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 8, 8, 120)    480         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 16, 16, 120)  0           concatenate_79[0][0]             \n",
      "                                                                 dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 8, 120)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 120)  480         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 8, 8, 8)      8640        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 16, 16, 120)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 8, 8, 8)      0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 8)    8640        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 8, 8, 128)    0           concatenate_58[0][0]             \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 16, 16, 8)    0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 8, 128)    512         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 16, 16, 128)  0           concatenate_80[0][0]             \n",
      "                                                                 dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 8, 128)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 128)  512         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 8, 8, 8)      9216        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 16, 16, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 8, 8, 8)      0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 8)    9216        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 8, 8, 136)    0           concatenate_59[0][0]             \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 16, 16, 8)    0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 8, 136)    544         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 16, 16, 136)  0           concatenate_81[0][0]             \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 8, 136)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 136)  544         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 8, 8, 8)      9792        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 16, 16, 136)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 8, 8, 8)      0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 8)    9792        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 8, 8, 144)    0           concatenate_60[0][0]             \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 16, 16, 8)    0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 8, 144)    576         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 16, 16, 144)  0           concatenate_82[0][0]             \n",
      "                                                                 dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 8, 144)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 144)  576         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 8, 8, 8)      10368       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 16, 16, 144)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 8, 8, 8)      0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 8)    10368       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 8, 8, 152)    0           concatenate_61[0][0]             \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 16, 16, 8)    0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 8, 152)    608         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 16, 16, 152)  0           concatenate_83[0][0]             \n",
      "                                                                 dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 8, 152)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 152)  608         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 8, 8)      10944       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 16, 16, 152)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 8, 8, 8)      0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 8)    10944       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 8, 8, 160)    0           concatenate_62[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 16, 16, 8)    0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 160)    640         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 16, 16, 160)  0           concatenate_84[0][0]             \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 8, 160)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 160)  640         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 8, 8, 8)      11520       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 16, 16, 160)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 8, 8, 8)      0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 8)    11520       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 8, 8, 168)    0           concatenate_63[0][0]             \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 16, 16, 8)    0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 168)    672         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 16, 16, 168)  0           concatenate_85[0][0]             \n",
      "                                                                 dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 8, 168)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 16, 16, 168)  672         concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 8, 8)      12096       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 16, 16, 168)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 8, 8, 8)      0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 8)    12096       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 8, 8, 176)    0           concatenate_64[0][0]             \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 16, 16, 8)    0           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 176)    704         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 16, 16, 176)  0           concatenate_86[0][0]             \n",
      "                                                                 dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 176)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 176)  704         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 8)      12672       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 16, 16, 176)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 8, 8, 8)      0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 8)    12672       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 8, 8, 184)    0           concatenate_65[0][0]             \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 16, 16, 8)    0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 184)    736         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 16, 16, 184)  0           concatenate_87[0][0]             \n",
      "                                                                 dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 184)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 12, 12, 8)    36800       concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 8)      1472        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 8)      1600        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 8, 8, 8)      0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 4, 8)      1600        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 8)      0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 4, 4, 16)     0           conv2d_95[0][0]                  \n",
      "                                                                 average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 16)     64          concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 4, 4, 16)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 4, 8)      128         activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 4, 4, 8)      0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 8)      0           dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 2, 2, 8)      32          average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 2, 8)      0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 2, 2, 8)      576         activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 2, 2, 8)      0           conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 2, 2, 16)     0           average_pooling2d_4[0][0]        \n",
      "                                                                 dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 2, 2, 16)     64          concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 2, 16)     0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 2, 2, 8)      1152        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 2, 2, 8)      0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 2, 2, 24)     0           concatenate_90[0][0]             \n",
      "                                                                 dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 2, 24)     96          concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 2, 24)     0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 2, 2, 8)      1728        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 2, 2, 8)      0           conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 2, 2, 32)     0           concatenate_91[0][0]             \n",
      "                                                                 dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 2, 32)     128         concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 2, 32)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 2, 8)      2304        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 2, 2, 8)      0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 2, 2, 40)     0           concatenate_92[0][0]             \n",
      "                                                                 dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 2, 40)     160         concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 2, 40)     0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 2, 8)      2880        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 2, 2, 8)      0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 2, 2, 48)     0           concatenate_93[0][0]             \n",
      "                                                                 dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 2, 48)     192         concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 2, 48)     0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 2, 8)      3456        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 2, 2, 8)      0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 2, 2, 56)     0           concatenate_94[0][0]             \n",
      "                                                                 dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 2, 56)     224         concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 2, 56)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 2, 8)      4032        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 2, 2, 8)      0           conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 2, 2, 64)     0           concatenate_95[0][0]             \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 2, 64)     256         concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 2, 2, 64)     0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 2, 8)      4608        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 2, 2, 8)      0           conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 2, 2, 72)     0           concatenate_96[0][0]             \n",
      "                                                                 dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 2, 72)     288         concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 2, 2, 72)     0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 2, 8)      5184        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 2, 2, 8)      0           conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 2, 2, 80)     0           concatenate_97[0][0]             \n",
      "                                                                 dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 2, 2, 80)     320         concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 2, 2, 80)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 2, 8)      5760        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 2, 2, 8)      0           conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 2, 2, 88)     0           concatenate_98[0][0]             \n",
      "                                                                 dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 2, 2, 88)     352         concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 2, 2, 88)     0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 2, 8)      6336        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 2, 2, 8)      0           conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 2, 2, 96)     0           concatenate_99[0][0]             \n",
      "                                                                 dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 2, 2, 96)     384         concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 2, 2, 96)     0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 2, 8)      6912        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 2, 2, 8)      0           conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 2, 2, 104)    0           concatenate_100[0][0]            \n",
      "                                                                 dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 2, 2, 104)    416         concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 2, 2, 104)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 2, 2, 8)      7488        activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 2, 2, 8)      0           conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 2, 2, 112)    0           concatenate_101[0][0]            \n",
      "                                                                 dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 2, 2, 112)    448         concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 2, 2, 112)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 2, 2, 8)      8064        activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 2, 2, 8)      0           conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 2, 2, 120)    0           concatenate_102[0][0]            \n",
      "                                                                 dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 2, 2, 120)    480         concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 2, 2, 120)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 2, 2, 8)      8640        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 2, 2, 8)      0           conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 2, 2, 128)    0           concatenate_103[0][0]            \n",
      "                                                                 dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 2, 2, 128)    512         concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 2, 2, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 2, 2, 8)      9216        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 2, 2, 8)      0           conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 2, 2, 136)    0           concatenate_104[0][0]            \n",
      "                                                                 dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 2, 2, 136)    544         concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 2, 2, 136)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 2, 2, 8)      9792        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 2, 2, 8)      0           conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 2, 2, 144)    0           concatenate_105[0][0]            \n",
      "                                                                 dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 2, 2, 144)    576         concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 2, 2, 144)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 2, 2, 8)      10368       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 2, 2, 8)      0           conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 2, 2, 152)    0           concatenate_106[0][0]            \n",
      "                                                                 dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 2, 2, 152)    608         concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 2, 2, 152)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 2, 2, 8)      10944       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 2, 2, 8)      0           conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 2, 2, 160)    0           concatenate_107[0][0]            \n",
      "                                                                 dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 2, 2, 160)    640         concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 2, 2, 160)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 2, 2, 8)      11520       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 2, 2, 8)      0           conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_109 (Concatenate)   (None, 2, 2, 168)    0           concatenate_108[0][0]            \n",
      "                                                                 dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 2, 2, 168)    672         concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 2, 2, 168)    0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 2, 2, 8)      12096       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 2, 2, 8)      0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_110 (Concatenate)   (None, 2, 2, 176)    0           concatenate_109[0][0]            \n",
      "                                                                 dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 2, 2, 176)    704         concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 2, 2, 176)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 2, 2, 8)      12672       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 2, 2, 8)      0           conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_111 (Concatenate)   (None, 2, 2, 184)    0           concatenate_110[0][0]            \n",
      "                                                                 dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 2, 2, 184)    736         concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 2, 2, 184)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 184)    0           activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 184)          0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1850        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 832,426\n",
      "Trainable params: 810,314\n",
      "Non-trainable params: 22,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQMBfM7rf6IU"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks For Early Stopping and Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qXt6uF-ZgEtG",
    "outputId": "17c39cad-e24a-40ba-d95c-05acd5777dda",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class EarlyStoppingByValidationAccuracy(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=1.0, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        accuracy_value= logs.get(self.monitor)\n",
    "       \n",
    "        if accuracy_value >= self.value:\n",
    "            self.model.stop_training = True\n",
    "            if self.verbose == 1:\n",
    "                print(\"Epoch %d: Threshold for early stopping has reached\" % (epoch + 1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks_list=[]\n",
    "model_save_path= \"best_model-CIFAR10-monimoy-my_computer2.h5\"\n",
    "callbacks_list.append(EarlyStoppingByValidationAccuracy(monitor='val_acc',  value=0.9201, verbose=1))\n",
    "callbacks_list.append(ModelCheckpoint(model_save_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model on the train data and calculate metrics (Training Loss, Training Accuracy, Validation Losss, Validation Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "52000/52000 [==============================] - 338s 7ms/step - loss: 1.9728 - acc: 0.2591 - val_loss: 2.3979 - val_acc: 0.2055\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.20550, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 2/250\n",
      "52000/52000 [==============================] - 305s 6ms/step - loss: 1.6869 - acc: 0.3636 - val_loss: 1.8543 - val_acc: 0.3592\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.20550 to 0.35920, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 3/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 1.5632 - acc: 0.4158 - val_loss: 1.6657 - val_acc: 0.4076\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.35920 to 0.40760, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 4/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 1.4708 - acc: 0.4556 - val_loss: 2.1125 - val_acc: 0.3503\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.40760\n",
      "Epoch 5/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 1.3925 - acc: 0.4900 - val_loss: 1.5036 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.40760 to 0.47370, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 6/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 1.3306 - acc: 0.5184 - val_loss: 1.4598 - val_acc: 0.4863\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.47370 to 0.48630, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 7/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 1.2738 - acc: 0.5388 - val_loss: 1.4807 - val_acc: 0.4921\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.48630 to 0.49210, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 8/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 1.2285 - acc: 0.5573 - val_loss: 1.7919 - val_acc: 0.4577\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.49210\n",
      "Epoch 9/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 1.1824 - acc: 0.5765 - val_loss: 1.6320 - val_acc: 0.4647\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.49210\n",
      "Epoch 10/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 1.1463 - acc: 0.5898 - val_loss: 1.6146 - val_acc: 0.4999\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.49210 to 0.49990, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 11/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 1.1156 - acc: 0.6045 - val_loss: 1.7500 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.49990\n",
      "Epoch 12/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 1.0906 - acc: 0.6128 - val_loss: 2.3382 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.49990\n",
      "Epoch 13/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 1.0628 - acc: 0.6212 - val_loss: 1.1799 - val_acc: 0.6033\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.49990 to 0.60330, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 14/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 1.0353 - acc: 0.6324 - val_loss: 1.5562 - val_acc: 0.5214\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60330\n",
      "Epoch 15/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 1.0178 - acc: 0.6399 - val_loss: 1.3332 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60330\n",
      "Epoch 16/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.9955 - acc: 0.6460 - val_loss: 1.5227 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.60330\n",
      "Epoch 17/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.9795 - acc: 0.6508 - val_loss: 1.2482 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.60330\n",
      "Epoch 18/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.9599 - acc: 0.6580 - val_loss: 1.0642 - val_acc: 0.6516\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.60330 to 0.65160, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 19/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.9456 - acc: 0.6663 - val_loss: 1.2597 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.65160\n",
      "Epoch 20/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.9261 - acc: 0.6717 - val_loss: 1.2080 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.65160\n",
      "Epoch 21/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.9108 - acc: 0.6755 - val_loss: 1.0950 - val_acc: 0.6530\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.65160 to 0.65300, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 22/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8968 - acc: 0.6796 - val_loss: 1.1563 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.65300\n",
      "Epoch 23/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8917 - acc: 0.6822 - val_loss: 1.3893 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.65300\n",
      "Epoch 24/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8764 - acc: 0.6862 - val_loss: 1.0794 - val_acc: 0.6484\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.65300\n",
      "Epoch 25/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8623 - acc: 0.6937 - val_loss: 0.9959 - val_acc: 0.6689\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.65300 to 0.66890, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 26/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8519 - acc: 0.6953 - val_loss: 1.0620 - val_acc: 0.6624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.66890\n",
      "Epoch 27/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8404 - acc: 0.7015 - val_loss: 1.2423 - val_acc: 0.6113\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.66890\n",
      "Epoch 28/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8385 - acc: 0.7013 - val_loss: 1.0614 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.66890\n",
      "Epoch 29/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8195 - acc: 0.7052 - val_loss: 1.3111 - val_acc: 0.6151\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.66890\n",
      "Epoch 30/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8165 - acc: 0.7087 - val_loss: 1.1445 - val_acc: 0.6402\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.66890\n",
      "Epoch 31/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.8031 - acc: 0.7146 - val_loss: 1.1903 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.66890\n",
      "Epoch 32/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.7944 - acc: 0.7171 - val_loss: 0.9859 - val_acc: 0.6837\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.66890 to 0.68370, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 33/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.7858 - acc: 0.7199 - val_loss: 0.9204 - val_acc: 0.7034\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.68370 to 0.70340, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 34/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.7827 - acc: 0.7217 - val_loss: 0.9935 - val_acc: 0.6760\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.70340\n",
      "Epoch 35/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7675 - acc: 0.7265 - val_loss: 1.1196 - val_acc: 0.6596\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.70340\n",
      "Epoch 36/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7578 - acc: 0.7308 - val_loss: 1.4209 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.70340\n",
      "Epoch 37/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7541 - acc: 0.7312 - val_loss: 0.9900 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.70340\n",
      "Epoch 38/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7456 - acc: 0.7349 - val_loss: 1.0011 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.70340\n",
      "Epoch 39/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7387 - acc: 0.7355 - val_loss: 0.8683 - val_acc: 0.7247\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.70340 to 0.72470, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 40/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7272 - acc: 0.7408 - val_loss: 0.9361 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.72470\n",
      "Epoch 41/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.7251 - acc: 0.7425 - val_loss: 0.8938 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.72470\n",
      "Epoch 42/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7174 - acc: 0.7453 - val_loss: 0.8866 - val_acc: 0.7198\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.72470\n",
      "Epoch 43/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.7081 - acc: 0.7498 - val_loss: 0.8511 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.72470\n",
      "Epoch 44/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.7044 - acc: 0.7497 - val_loss: 0.9610 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.72470\n",
      "Epoch 45/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6994 - acc: 0.7528 - val_loss: 1.0481 - val_acc: 0.6828\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.72470\n",
      "Epoch 46/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.6894 - acc: 0.7568 - val_loss: 0.8433 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.72470 to 0.72540, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 47/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6847 - acc: 0.7572 - val_loss: 1.0897 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.72540\n",
      "Epoch 48/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6772 - acc: 0.7602 - val_loss: 1.1594 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.72540\n",
      "Epoch 49/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6697 - acc: 0.7624 - val_loss: 0.8276 - val_acc: 0.7343\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.72540 to 0.73430, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 50/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6696 - acc: 0.7630 - val_loss: 1.0513 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.73430\n",
      "Epoch 51/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6595 - acc: 0.7673 - val_loss: 1.1138 - val_acc: 0.6704\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.73430\n",
      "Epoch 52/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6551 - acc: 0.7679 - val_loss: 0.9388 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.73430\n",
      "Epoch 53/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.6495 - acc: 0.7709 - val_loss: 0.7990 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.73430 to 0.74760, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 54/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6439 - acc: 0.7734 - val_loss: 0.8563 - val_acc: 0.7315\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.74760\n",
      "Epoch 55/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6397 - acc: 0.7743 - val_loss: 0.7665 - val_acc: 0.7529\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.74760 to 0.75290, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 56/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6361 - acc: 0.7747 - val_loss: 0.9348 - val_acc: 0.7124\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.75290\n",
      "Epoch 57/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6293 - acc: 0.7779 - val_loss: 0.9758 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.75290\n",
      "Epoch 58/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6206 - acc: 0.7810 - val_loss: 0.7848 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.75290 to 0.75320, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 59/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6187 - acc: 0.7837 - val_loss: 0.7948 - val_acc: 0.7511\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.75320\n",
      "Epoch 60/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6076 - acc: 0.7860 - val_loss: 0.7716 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.75320\n",
      "Epoch 61/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6041 - acc: 0.7884 - val_loss: 0.8124 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.75320\n",
      "Epoch 62/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.6013 - acc: 0.7893 - val_loss: 0.8012 - val_acc: 0.7526\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.75320\n",
      "Epoch 63/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5989 - acc: 0.7907 - val_loss: 0.7580 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.75320 to 0.76500, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 64/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5952 - acc: 0.7915 - val_loss: 0.7723 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76500\n",
      "Epoch 65/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5858 - acc: 0.7937 - val_loss: 1.0074 - val_acc: 0.7177\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76500\n",
      "Epoch 66/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5863 - acc: 0.7948 - val_loss: 0.7404 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.76500 to 0.76610, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 67/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5806 - acc: 0.7955 - val_loss: 0.7716 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76610\n",
      "Epoch 68/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5764 - acc: 0.7987 - val_loss: 0.7087 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.76610 to 0.77680, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 69/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5684 - acc: 0.8016 - val_loss: 0.7959 - val_acc: 0.7546\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.77680\n",
      "Epoch 70/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5640 - acc: 0.8064 - val_loss: 0.7652 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.77680\n",
      "Epoch 71/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5642 - acc: 0.8023 - val_loss: 0.7716 - val_acc: 0.7607\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.77680\n",
      "Epoch 72/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5568 - acc: 0.8055 - val_loss: 0.9454 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.77680\n",
      "Epoch 73/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5528 - acc: 0.8070 - val_loss: 0.6390 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.77680 to 0.79500, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 74/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5520 - acc: 0.8063 - val_loss: 0.7297 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.79500\n",
      "Epoch 75/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5484 - acc: 0.8094 - val_loss: 0.9519 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.79500\n",
      "Epoch 76/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5442 - acc: 0.8111 - val_loss: 0.8749 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.79500\n",
      "Epoch 77/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5374 - acc: 0.8118 - val_loss: 0.7850 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.79500\n",
      "Epoch 78/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5291 - acc: 0.8154 - val_loss: 0.8043 - val_acc: 0.7615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00078: val_acc did not improve from 0.79500\n",
      "Epoch 79/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5294 - acc: 0.8159 - val_loss: 0.7955 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.79500\n",
      "Epoch 80/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5233 - acc: 0.8164 - val_loss: 0.6713 - val_acc: 0.7888\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.79500\n",
      "Epoch 81/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5249 - acc: 0.8179 - val_loss: 0.7655 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.79500\n",
      "Epoch 82/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5246 - acc: 0.8166 - val_loss: 0.7717 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.79500\n",
      "Epoch 83/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.5162 - acc: 0.8208 - val_loss: 0.7006 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.79500\n",
      "Epoch 84/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5162 - acc: 0.8194 - val_loss: 0.9245 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.79500\n",
      "Epoch 85/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.5080 - acc: 0.8245 - val_loss: 0.6666 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.79500\n",
      "Epoch 86/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5042 - acc: 0.8247 - val_loss: 0.6934 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.79500\n",
      "Epoch 87/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.5028 - acc: 0.8251 - val_loss: 0.7712 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.79500\n",
      "Epoch 88/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4998 - acc: 0.8233 - val_loss: 0.7130 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.79500\n",
      "Epoch 89/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4955 - acc: 0.8257 - val_loss: 0.6998 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.79500\n",
      "Epoch 90/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.4958 - acc: 0.8270 - val_loss: 0.7126 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.79500\n",
      "Epoch 91/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4940 - acc: 0.8284 - val_loss: 0.7566 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.79500\n",
      "Epoch 92/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4809 - acc: 0.8324 - val_loss: 1.0565 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.79500\n",
      "Epoch 93/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4803 - acc: 0.8304 - val_loss: 0.6450 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.79500 to 0.80770, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 94/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4807 - acc: 0.8334 - val_loss: 0.8656 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.80770\n",
      "Epoch 95/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4751 - acc: 0.8338 - val_loss: 0.6801 - val_acc: 0.7924\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.80770\n",
      "Epoch 96/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4777 - acc: 0.8359 - val_loss: 0.7563 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.80770\n",
      "Epoch 97/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4734 - acc: 0.8350 - val_loss: 0.7217 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.80770\n",
      "Epoch 98/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.4750 - acc: 0.8342 - val_loss: 0.6739 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.80770\n",
      "Epoch 99/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.4669 - acc: 0.8384 - val_loss: 0.7757 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.80770\n",
      "Epoch 100/250\n",
      "52000/52000 [==============================] - 316s 6ms/step - loss: 0.4616 - acc: 0.8396 - val_loss: 0.7094 - val_acc: 0.7863\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.80770\n",
      "Epoch 101/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.4661 - acc: 0.8390 - val_loss: 0.7241 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.80770\n",
      "Epoch 102/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4626 - acc: 0.8388 - val_loss: 0.7888 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.80770\n",
      "Epoch 103/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4558 - acc: 0.8396 - val_loss: 0.7287 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.80770\n",
      "Epoch 104/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.4590 - acc: 0.8400 - val_loss: 0.7720 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.80770\n",
      "Epoch 105/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.4488 - acc: 0.8447 - val_loss: 0.7493 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.80770\n",
      "Epoch 106/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4494 - acc: 0.8440 - val_loss: 0.7835 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.80770\n",
      "Epoch 107/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4498 - acc: 0.8446 - val_loss: 0.7648 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.80770\n",
      "Epoch 108/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4459 - acc: 0.8445 - val_loss: 0.8044 - val_acc: 0.7773\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.80770\n",
      "Epoch 109/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4370 - acc: 0.8474 - val_loss: 0.7503 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.80770\n",
      "Epoch 110/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4399 - acc: 0.8475 - val_loss: 0.7332 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.80770\n",
      "Epoch 111/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4419 - acc: 0.8464 - val_loss: 0.7286 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.80770\n",
      "Epoch 112/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4353 - acc: 0.8486 - val_loss: 0.6650 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.80770\n",
      "Epoch 113/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4293 - acc: 0.8518 - val_loss: 0.7144 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.80770\n",
      "Epoch 114/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4377 - acc: 0.8504 - val_loss: 0.7682 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.80770\n",
      "Epoch 115/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4248 - acc: 0.8515 - val_loss: 0.7151 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.80770\n",
      "Epoch 116/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4245 - acc: 0.8525 - val_loss: 0.6815 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.80770\n",
      "Epoch 117/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4207 - acc: 0.8539 - val_loss: 0.6779 - val_acc: 0.8008\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.80770\n",
      "Epoch 118/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4216 - acc: 0.8542 - val_loss: 0.6941 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.80770\n",
      "Epoch 119/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.4202 - acc: 0.8522 - val_loss: 0.8741 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.80770\n",
      "Epoch 120/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4161 - acc: 0.8546 - val_loss: 0.7077 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.80770\n",
      "Epoch 121/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4173 - acc: 0.8553 - val_loss: 0.9042 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.80770\n",
      "Epoch 122/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4114 - acc: 0.8573 - val_loss: 0.7042 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.80770\n",
      "Epoch 123/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.4092 - acc: 0.8582 - val_loss: 0.7627 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.80770\n",
      "Epoch 124/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.4133 - acc: 0.8555 - val_loss: 0.6535 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.80770\n",
      "Epoch 125/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.4092 - acc: 0.8552 - val_loss: 0.6935 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.80770\n",
      "Epoch 126/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.4029 - acc: 0.8596 - val_loss: 0.6755 - val_acc: 0.8110\n",
      "\n",
      "Epoch 00126: val_acc improved from 0.80770 to 0.81100, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 127/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.4009 - acc: 0.8608 - val_loss: 0.6586 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.81100\n",
      "Epoch 128/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.3985 - acc: 0.8619 - val_loss: 0.8238 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.81100\n",
      "Epoch 129/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.3979 - acc: 0.8610 - val_loss: 0.7133 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.81100\n",
      "Epoch 130/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.3953 - acc: 0.8622 - val_loss: 0.8598 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.81100\n",
      "Epoch 131/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.3961 - acc: 0.8618 - val_loss: 0.6392 - val_acc: 0.8121\n",
      "\n",
      "Epoch 00131: val_acc improved from 0.81100 to 0.81210, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 132/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3937 - acc: 0.8637 - val_loss: 0.6839 - val_acc: 0.8083\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.81210\n",
      "Epoch 133/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3909 - acc: 0.8639 - val_loss: 0.7585 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.81210\n",
      "Epoch 134/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3874 - acc: 0.8642 - val_loss: 0.6672 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00134: val_acc improved from 0.81210 to 0.81610, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 135/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3876 - acc: 0.8629 - val_loss: 0.8263 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.81610\n",
      "Epoch 136/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3884 - acc: 0.8654 - val_loss: 0.9225 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.81610\n",
      "Epoch 137/250\n",
      "52000/52000 [==============================] - 311s 6ms/step - loss: 0.3803 - acc: 0.8661 - val_loss: 0.6696 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.81610\n",
      "Epoch 138/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3822 - acc: 0.8679 - val_loss: 0.6054 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00138: val_acc improved from 0.81610 to 0.82030, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 139/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3846 - acc: 0.8655 - val_loss: 0.6693 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.82030\n",
      "Epoch 140/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3764 - acc: 0.8682 - val_loss: 0.6650 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.82030\n",
      "Epoch 141/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3775 - acc: 0.8679 - val_loss: 0.7714 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.82030\n",
      "Epoch 142/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3742 - acc: 0.8694 - val_loss: 0.6410 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.82030\n",
      "Epoch 143/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.3734 - acc: 0.8693 - val_loss: 0.7013 - val_acc: 0.8043\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.82030\n",
      "Epoch 144/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3672 - acc: 0.8718 - val_loss: 0.6085 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.82030 to 0.82210, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 145/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3710 - acc: 0.8713 - val_loss: 0.7217 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.82210\n",
      "Epoch 146/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3645 - acc: 0.8726 - val_loss: 0.7359 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.82210\n",
      "Epoch 147/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3648 - acc: 0.8725 - val_loss: 0.7572 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.82210\n",
      "Epoch 148/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3649 - acc: 0.8744 - val_loss: 0.6894 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.82210\n",
      "Epoch 149/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3623 - acc: 0.8741 - val_loss: 0.8338 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.82210\n",
      "Epoch 150/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3625 - acc: 0.8721 - val_loss: 0.9611 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.82210\n",
      "Epoch 151/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3637 - acc: 0.8735 - val_loss: 0.6656 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.82210\n",
      "Epoch 152/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3618 - acc: 0.8749 - val_loss: 0.6713 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.82210\n",
      "Epoch 153/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3575 - acc: 0.8749 - val_loss: 0.6686 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.82210\n",
      "Epoch 154/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3498 - acc: 0.8777 - val_loss: 0.7032 - val_acc: 0.8084\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.82210\n",
      "Epoch 155/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3535 - acc: 0.8782 - val_loss: 0.7043 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.82210\n",
      "Epoch 156/250\n",
      "52000/52000 [==============================] - 306s 6ms/step - loss: 0.3512 - acc: 0.8769 - val_loss: 0.6533 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.82210\n",
      "Epoch 157/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3497 - acc: 0.8788 - val_loss: 0.7604 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.82210\n",
      "Epoch 158/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3507 - acc: 0.8785 - val_loss: 0.7681 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.82210\n",
      "Epoch 159/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.3438 - acc: 0.8783 - val_loss: 0.6072 - val_acc: 0.8278\n",
      "\n",
      "Epoch 00159: val_acc improved from 0.82210 to 0.82780, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 160/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3456 - acc: 0.8789 - val_loss: 0.6374 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.82780\n",
      "Epoch 161/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.3444 - acc: 0.8796 - val_loss: 0.7417 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.82780\n",
      "Epoch 162/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3493 - acc: 0.8785 - val_loss: 0.7009 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.82780\n",
      "Epoch 163/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3417 - acc: 0.8804 - val_loss: 0.6764 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.82780\n",
      "Epoch 164/250\n",
      "52000/52000 [==============================] - 306s 6ms/step - loss: 0.3418 - acc: 0.8824 - val_loss: 0.6696 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.82780\n",
      "Epoch 165/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3357 - acc: 0.8848 - val_loss: 0.7290 - val_acc: 0.8064\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.82780\n",
      "Epoch 166/250\n",
      "52000/52000 [==============================] - 307s 6ms/step - loss: 0.3355 - acc: 0.8829 - val_loss: 0.7051 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.82780\n",
      "Epoch 167/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3405 - acc: 0.8812 - val_loss: 0.7857 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.82780\n",
      "Epoch 168/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3345 - acc: 0.8823 - val_loss: 0.6496 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.82780\n",
      "Epoch 169/250\n",
      "52000/52000 [==============================] - 309s 6ms/step - loss: 0.3352 - acc: 0.8823 - val_loss: 0.7817 - val_acc: 0.7977\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.82780\n",
      "Epoch 170/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3339 - acc: 0.8835 - val_loss: 0.7151 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.82780\n",
      "Epoch 171/250\n",
      "52000/52000 [==============================] - 308s 6ms/step - loss: 0.3326 - acc: 0.8845 - val_loss: 0.6817 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.82780\n",
      "Epoch 172/250\n",
      "52000/52000 [==============================] - 314s 6ms/step - loss: 0.3333 - acc: 0.8841 - val_loss: 0.6831 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.82780\n",
      "Epoch 173/250\n",
      "52000/52000 [==============================] - 310s 6ms/step - loss: 0.3276 - acc: 0.8853 - val_loss: 0.6867 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.82780\n",
      "Epoch 174/250\n",
      "52000/52000 [==============================] - 316s 6ms/step - loss: 0.3249 - acc: 0.8865 - val_loss: 0.7793 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.82780\n",
      "Epoch 175/250\n",
      "52000/52000 [==============================] - 313s 6ms/step - loss: 0.3296 - acc: 0.8850 - val_loss: 0.7977 - val_acc: 0.7992\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.82780\n",
      "Epoch 176/250\n",
      "52000/52000 [==============================] - 312s 6ms/step - loss: 0.3216 - acc: 0.8882 - val_loss: 0.6893 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.82780\n",
      "Epoch 177/250\n",
      "52000/52000 [==============================] - 432s 8ms/step - loss: 0.3283 - acc: 0.8843 - val_loss: 0.6504 - val_acc: 0.8255\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.82780\n",
      "Epoch 178/250\n",
      "52000/52000 [==============================] - 1133s 22ms/step - loss: 0.3212 - acc: 0.8874 - val_loss: 0.7545 - val_acc: 0.8038\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.82780\n",
      "Epoch 179/250\n",
      "52000/52000 [==============================] - 1413s 27ms/step - loss: 0.3217 - acc: 0.8880 - val_loss: 0.7309 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.82780\n",
      "Epoch 180/250\n",
      "52000/52000 [==============================] - 1415s 27ms/step - loss: 0.3185 - acc: 0.8888 - val_loss: 0.6532 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.82780\n",
      "Epoch 181/250\n",
      "52000/52000 [==============================] - 1419s 27ms/step - loss: 0.3186 - acc: 0.8882 - val_loss: 0.6701 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.82780\n",
      "Epoch 182/250\n",
      "52000/52000 [==============================] - 1417s 27ms/step - loss: 0.3152 - acc: 0.8901 - val_loss: 0.6753 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.82780\n",
      "Epoch 183/250\n",
      "52000/52000 [==============================] - 1421s 27ms/step - loss: 0.3159 - acc: 0.8906 - val_loss: 0.6294 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.82780\n",
      "Epoch 184/250\n",
      "52000/52000 [==============================] - 1416s 27ms/step - loss: 0.3130 - acc: 0.8888 - val_loss: 0.8249 - val_acc: 0.7934\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.82780\n",
      "Epoch 185/250\n",
      "52000/52000 [==============================] - 1423s 27ms/step - loss: 0.3079 - acc: 0.8930 - val_loss: 0.6550 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.82780\n",
      "Epoch 186/250\n",
      "52000/52000 [==============================] - 1421s 27ms/step - loss: 0.3084 - acc: 0.8923 - val_loss: 0.7866 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.82780\n",
      "Epoch 187/250\n",
      "52000/52000 [==============================] - 1422s 27ms/step - loss: 0.3078 - acc: 0.8917 - val_loss: 0.7287 - val_acc: 0.8084\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.82780\n",
      "Epoch 188/250\n",
      "52000/52000 [==============================] - 1417s 27ms/step - loss: 0.3128 - acc: 0.8890 - val_loss: 0.7207 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.82780\n",
      "Epoch 189/250\n",
      "52000/52000 [==============================] - 1421s 27ms/step - loss: 0.3089 - acc: 0.8910 - val_loss: 0.6472 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.82780\n",
      "Epoch 190/250\n",
      "52000/52000 [==============================] - 1427s 27ms/step - loss: 0.3067 - acc: 0.8928 - val_loss: 0.6112 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00190: val_acc improved from 0.82780 to 0.83050, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 191/250\n",
      "52000/52000 [==============================] - 1420s 27ms/step - loss: 0.3055 - acc: 0.8928 - val_loss: 0.6762 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.83050\n",
      "Epoch 192/250\n",
      "52000/52000 [==============================] - 1422s 27ms/step - loss: 0.3052 - acc: 0.8934 - val_loss: 0.6461 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.83050\n",
      "Epoch 193/250\n",
      "52000/52000 [==============================] - 1422s 27ms/step - loss: 0.3025 - acc: 0.8947 - val_loss: 0.7120 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.83050\n",
      "Epoch 194/250\n",
      "52000/52000 [==============================] - 1421s 27ms/step - loss: 0.2994 - acc: 0.8952 - val_loss: 0.6361 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00194: val_acc improved from 0.83050 to 0.83240, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 195/250\n",
      "52000/52000 [==============================] - 1422s 27ms/step - loss: 0.3048 - acc: 0.8934 - val_loss: 0.6471 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.83240\n",
      "Epoch 196/250\n",
      "52000/52000 [==============================] - 1412s 27ms/step - loss: 0.3036 - acc: 0.8934 - val_loss: 0.6592 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.83240\n",
      "Epoch 197/250\n",
      "52000/52000 [==============================] - 1415s 27ms/step - loss: 0.3003 - acc: 0.8936 - val_loss: 0.6935 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.83240\n",
      "Epoch 198/250\n",
      "52000/52000 [==============================] - 1418s 27ms/step - loss: 0.2978 - acc: 0.8964 - val_loss: 0.6678 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.83240\n",
      "Epoch 199/250\n",
      "52000/52000 [==============================] - 1404s 27ms/step - loss: 0.2968 - acc: 0.8961 - val_loss: 0.7043 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.83240\n",
      "Epoch 200/250\n",
      "52000/52000 [==============================] - 1430s 28ms/step - loss: 0.2949 - acc: 0.8970 - val_loss: 0.6733 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.83240\n",
      "Epoch 201/250\n",
      "52000/52000 [==============================] - 1431s 28ms/step - loss: 0.2953 - acc: 0.8976 - val_loss: 0.6423 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.83240\n",
      "Epoch 202/250\n",
      "52000/52000 [==============================] - 1433s 28ms/step - loss: 0.2962 - acc: 0.8964 - val_loss: 0.6661 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.83240\n",
      "Epoch 203/250\n",
      "52000/52000 [==============================] - 1431s 28ms/step - loss: 0.2916 - acc: 0.8980 - val_loss: 0.6974 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.83240\n",
      "Epoch 204/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 1430s 28ms/step - loss: 0.2879 - acc: 0.8993 - val_loss: 0.7310 - val_acc: 0.8137\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.83240\n",
      "Epoch 205/250\n",
      "52000/52000 [==============================] - 1430s 27ms/step - loss: 0.2863 - acc: 0.8983 - val_loss: 0.6501 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.83240\n",
      "Epoch 206/250\n",
      "52000/52000 [==============================] - 1432s 28ms/step - loss: 0.2883 - acc: 0.8983 - val_loss: 0.8418 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.83240\n",
      "Epoch 207/250\n",
      "52000/52000 [==============================] - 1414s 27ms/step - loss: 0.2894 - acc: 0.8981 - val_loss: 0.6856 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.83240\n",
      "Epoch 208/250\n",
      "52000/52000 [==============================] - 1408s 27ms/step - loss: 0.2851 - acc: 0.9014 - val_loss: 0.6746 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.83240\n",
      "Epoch 209/250\n",
      "52000/52000 [==============================] - 1414s 27ms/step - loss: 0.2898 - acc: 0.8979 - val_loss: 0.7589 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.83240\n",
      "Epoch 210/250\n",
      "52000/52000 [==============================] - 1413s 27ms/step - loss: 0.2861 - acc: 0.8987 - val_loss: 0.6772 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.83240\n",
      "Epoch 211/250\n",
      "52000/52000 [==============================] - 1410s 27ms/step - loss: 0.2855 - acc: 0.8995 - val_loss: 0.6554 - val_acc: 0.8308\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.83240\n",
      "Epoch 212/250\n",
      "52000/52000 [==============================] - 1410s 27ms/step - loss: 0.2802 - acc: 0.9024 - val_loss: 0.6200 - val_acc: 0.8346\n",
      "\n",
      "Epoch 00212: val_acc improved from 0.83240 to 0.83460, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 213/250\n",
      "52000/52000 [==============================] - 1415s 27ms/step - loss: 0.2812 - acc: 0.9010 - val_loss: 0.6372 - val_acc: 0.8287\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.83460\n",
      "Epoch 214/250\n",
      "52000/52000 [==============================] - 1414s 27ms/step - loss: 0.2826 - acc: 0.9014 - val_loss: 0.6412 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.83460\n",
      "Epoch 215/250\n",
      "52000/52000 [==============================] - 1411s 27ms/step - loss: 0.2817 - acc: 0.9014 - val_loss: 0.7844 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.83460\n",
      "Epoch 216/250\n",
      "52000/52000 [==============================] - 1414s 27ms/step - loss: 0.2779 - acc: 0.9025 - val_loss: 0.6163 - val_acc: 0.8345\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.83460\n",
      "Epoch 217/250\n",
      "52000/52000 [==============================] - 1413s 27ms/step - loss: 0.2783 - acc: 0.9020 - val_loss: 0.6153 - val_acc: 0.8380\n",
      "\n",
      "Epoch 00217: val_acc improved from 0.83460 to 0.83800, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 218/250\n",
      "52000/52000 [==============================] - 1410s 27ms/step - loss: 0.2749 - acc: 0.9017 - val_loss: 0.6882 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.83800\n",
      "Epoch 219/250\n",
      "52000/52000 [==============================] - 1407s 27ms/step - loss: 0.2730 - acc: 0.9037 - val_loss: 0.6908 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.83800\n",
      "Epoch 220/250\n",
      "52000/52000 [==============================] - 1404s 27ms/step - loss: 0.2745 - acc: 0.9037 - val_loss: 0.7413 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.83800\n",
      "Epoch 221/250\n",
      "52000/52000 [==============================] - 1394s 27ms/step - loss: 0.2711 - acc: 0.9048 - val_loss: 0.6093 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00221: val_acc improved from 0.83800 to 0.83910, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 222/250\n",
      "52000/52000 [==============================] - 1394s 27ms/step - loss: 0.2774 - acc: 0.9022 - val_loss: 0.7358 - val_acc: 0.8096\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.83910\n",
      "Epoch 223/250\n",
      "52000/52000 [==============================] - 1392s 27ms/step - loss: 0.2730 - acc: 0.9024 - val_loss: 0.5887 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00223: val_acc improved from 0.83910 to 0.84350, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 224/250\n",
      "52000/52000 [==============================] - 1389s 27ms/step - loss: 0.2736 - acc: 0.9038 - val_loss: 0.6335 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.84350\n",
      "Epoch 225/250\n",
      "52000/52000 [==============================] - 1397s 27ms/step - loss: 0.2721 - acc: 0.9047 - val_loss: 0.6920 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.84350\n",
      "Epoch 226/250\n",
      "52000/52000 [==============================] - 1388s 27ms/step - loss: 0.2653 - acc: 0.9070 - val_loss: 0.6576 - val_acc: 0.8276\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.84350\n",
      "Epoch 227/250\n",
      "52000/52000 [==============================] - 1392s 27ms/step - loss: 0.2698 - acc: 0.9043 - val_loss: 0.7497 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.84350\n",
      "Epoch 228/250\n",
      "52000/52000 [==============================] - 1390s 27ms/step - loss: 0.2662 - acc: 0.9057 - val_loss: 0.6914 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.84350\n",
      "Epoch 229/250\n",
      "52000/52000 [==============================] - 1393s 27ms/step - loss: 0.2670 - acc: 0.9063 - val_loss: 0.5704 - val_acc: 0.8432\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.84350\n",
      "Epoch 230/250\n",
      "52000/52000 [==============================] - 1387s 27ms/step - loss: 0.2688 - acc: 0.9064 - val_loss: 0.6132 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.84350\n",
      "Epoch 231/250\n",
      "52000/52000 [==============================] - 1399s 27ms/step - loss: 0.2662 - acc: 0.9067 - val_loss: 0.7232 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.84350\n",
      "Epoch 232/250\n",
      "52000/52000 [==============================] - 1398s 27ms/step - loss: 0.2631 - acc: 0.9070 - val_loss: 0.6383 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.84350\n",
      "Epoch 233/250\n",
      "52000/52000 [==============================] - 1423s 27ms/step - loss: 0.2624 - acc: 0.9075 - val_loss: 0.6322 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.84350\n",
      "Epoch 234/250\n",
      "52000/52000 [==============================] - 1394s 27ms/step - loss: 0.2621 - acc: 0.9091 - val_loss: 0.5805 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00234: val_acc improved from 0.84350 to 0.84530, saving model to best_model-CIFAR10-monimoy-my_computer2.h5\n",
      "Epoch 235/250\n",
      "52000/52000 [==============================] - 1411s 27ms/step - loss: 0.2617 - acc: 0.9082 - val_loss: 0.8627 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.84530\n",
      "Epoch 236/250\n",
      "52000/52000 [==============================] - 1405s 27ms/step - loss: 0.2621 - acc: 0.9088 - val_loss: 0.6658 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.84530\n",
      "Epoch 237/250\n",
      "52000/52000 [==============================] - 1409s 27ms/step - loss: 0.2622 - acc: 0.9076 - val_loss: 0.6264 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.84530\n",
      "Epoch 238/250\n",
      "52000/52000 [==============================] - 1409s 27ms/step - loss: 0.2553 - acc: 0.9101 - val_loss: 0.6778 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.84530\n",
      "Epoch 239/250\n",
      "52000/52000 [==============================] - 1406s 27ms/step - loss: 0.2597 - acc: 0.9110 - val_loss: 0.7013 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.84530\n",
      "Epoch 240/250\n",
      "52000/52000 [==============================] - 1404s 27ms/step - loss: 0.2586 - acc: 0.9096 - val_loss: 0.7386 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.84530\n",
      "Epoch 241/250\n",
      "52000/52000 [==============================] - 1402s 27ms/step - loss: 0.2543 - acc: 0.9110 - val_loss: 0.7649 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.84530\n",
      "Epoch 242/250\n",
      "52000/52000 [==============================] - 1402s 27ms/step - loss: 0.2503 - acc: 0.9122 - val_loss: 0.6072 - val_acc: 0.8424\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.84530\n",
      "Epoch 243/250\n",
      "52000/52000 [==============================] - 1404s 27ms/step - loss: 0.2537 - acc: 0.9108 - val_loss: 0.7165 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.84530\n",
      "Epoch 244/250\n",
      "52000/52000 [==============================] - 1407s 27ms/step - loss: 0.2555 - acc: 0.9102 - val_loss: 0.7177 - val_acc: 0.8230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00244: val_acc did not improve from 0.84530\n",
      "Epoch 245/250\n",
      "52000/52000 [==============================] - 1401s 27ms/step - loss: 0.2518 - acc: 0.9121 - val_loss: 0.6262 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.84530\n",
      "Epoch 246/250\n",
      "52000/52000 [==============================] - 1403s 27ms/step - loss: 0.2515 - acc: 0.9132 - val_loss: 0.7264 - val_acc: 0.8225\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.84530\n",
      "Epoch 247/250\n",
      "52000/52000 [==============================] - 1403s 27ms/step - loss: 0.2509 - acc: 0.9113 - val_loss: 0.7642 - val_acc: 0.8143\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.84530\n",
      "Epoch 248/250\n",
      "52000/52000 [==============================] - 1409s 27ms/step - loss: 0.2499 - acc: 0.9127 - val_loss: 0.6000 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.84530\n",
      "Epoch 249/250\n",
      "52000/52000 [==============================] - 1403s 27ms/step - loss: 0.2509 - acc: 0.9118 - val_loss: 0.5953 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.84530\n",
      "Epoch 250/250\n",
      "52000/52000 [==============================] - 1403s 27ms/step - loss: 0.2541 - acc: 0.9108 - val_loss: 0.5730 - val_acc: 0.8451\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.84530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27fc1c35f28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step\n",
      "Test loss: 0.5730056409597397\n",
      "Test accuracy: 0.8451\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "On the CIFAR image dataset using Keras based Convolution Neural Network I have achieved testing accuracy of 84.51%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DNST_CIFAR10_monimoy_changes_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
